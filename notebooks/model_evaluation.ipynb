{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    ")\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import yaml\n",
    "from ipywidgets import interact, widgets, VBox\n",
    "from typing import List, Tuple, Dict, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Go up one directory level from the notebook's location\n",
    "project_root = Path().resolve().parent  # Navigate to the parent directory\n",
    "os.chdir(project_root)  # Set this as the working directory\n",
    "\n",
    "# print(\"Current working directory set to:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC: models\\classification\\SVC\n",
      "RandomForest: models\\classification\\RandomForestClassifier\n",
      "KernelRidge: models\\regression\\KernelRidge\n"
     ]
    }
   ],
   "source": [
    "with open(\"config/paths.yaml\", \"r\") as file:\n",
    "    paths = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "# Load the data\n",
    "X = pd.read_csv(paths[\"train\"][\"final_features\"])\n",
    "y = pd.read_csv(paths[\"train\"][\"labels\"])\n",
    "X_val = pd.read_csv(paths[\"val\"][\"final_features\"])\n",
    "y_val = pd.read_csv(paths[\"val\"][\"labels\"])\n",
    "# test_X = pd.read_csv(paths[\"test\"][\"final_features\"])\n",
    "\n",
    "clf_mask = pd.read_csv(paths[\"clf_mask_file\"], index_col=\"label\")\n",
    "regr_mask = pd.read_csv(paths[\"regr_mask_file\"], index_col=\"label\")\n",
    "\n",
    "# Path to stored classification models\n",
    "svc_models_path =  os.path.normpath(os.path.join(paths[\"models\"][\"clf\"], \"SVC\"))\n",
    "rf_models_path = os.path.normpath(\n",
    "    os.path.join(paths[\"models\"][\"clf\"], \"RandomForestClassifier\")\n",
    ")\n",
    "\n",
    "# Path to stored regression models\n",
    "kr_models_path =  os.path.normpath(os.path.join(paths[\"models\"][\"regr\"], \"KernelRidge\"))\n",
    "\n",
    "print(f\"SVC: {svc_models_path}\")\n",
    "print(f\"RandomForest: {rf_models_path}\")\n",
    "print(f\"KernelRidge: {kr_models_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC models: 11\n",
      "RandomForest models: 11\n",
      "KernelRidge models: 4\n"
     ]
    }
   ],
   "source": [
    "def load_models(dir: str) -> Dict:\n",
    "    models = {}\n",
    "    for file in os.listdir(dir):\n",
    "        if file.endswith(\".joblib\"):\n",
    "            model = joblib.load(os.path.join(dir, file))\n",
    "            label = file.split(\".\")[0]\n",
    "            models[label] = model\n",
    "    return models\n",
    "\n",
    "svc_models = load_models(svc_models_path)\n",
    "rf_models = load_models(rf_models_path)\n",
    "kr_models = load_models(kr_models_path)\n",
    "\n",
    "print(f\"SVC models: {len(svc_models)}\")\n",
    "print(f\"RandomForest models: {len(rf_models)}\")\n",
    "print(f\"KernelRidge models: {len(kr_models)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.helper import standardize_data\n",
    "\n",
    "\n",
    "def predict(X_train, X_val, feature_mask, model):\n",
    "    X_train_selected = X_train.loc[:, feature_mask]\n",
    "    X_val_selected = X_val.loc[:, feature_mask]\n",
    "\n",
    "    norm_X_train, norm_X_val = standardize_data(X_train_selected, X_val_selected)\n",
    "\n",
    "    y_pred = model.predict(norm_X_val)\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clf_metrics(\n",
    "    X: pd.DataFrame,\n",
    "    X_val: pd.DataFrame,\n",
    "    y_val: pd.DataFrame,\n",
    "    mask: pd.DataFrame,\n",
    "    models: Dict,\n",
    "    out_file: str,\n",
    ") -> Dict:\n",
    "\n",
    "    metrics = {}\n",
    "\n",
    "    X = X.drop(columns=[\"pid\"])\n",
    "    X_val = X_val.drop(columns=[\"pid\"])\n",
    "\n",
    "    for label, model in models.items():\n",
    "        y_true = y_val[label]\n",
    "        label_mask = mask.loc[label, :]\n",
    "        y_pred = predict(X, X_val, label_mask, model)\n",
    "\n",
    "        # Compute metrics\n",
    "        acc = accuracy_score(y_true, y_pred)\n",
    "        prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "        rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "        f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "        metrics[label] = {\n",
    "            \"Accuracy\": acc,\n",
    "            \"Precision\": prec,\n",
    "            \"Recall\": rec,\n",
    "            \"F1 Score\": f1,\n",
    "            \"Confusion\": cm,\n",
    "        }\n",
    "\n",
    "    # Convert metrics to a DataFrame for summary\n",
    "    metrics_df = pd.DataFrame(metrics)\n",
    "    metrics_df.to_csv(out_file)\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC: {'LABEL_Alkalinephos': {'Accuracy': 0.7141352987628323, 'Precision': np.float64(0.44017632241813603), 'Recall': np.float64(0.7801339285714286), 'F1 Score': np.float64(0.5628019323671497), 'Confusion': array([[2014,  889],\n",
      "       [ 197,  699]])}, 'LABEL_AST': {'Accuracy': 0.7078178468017899, 'Precision': np.float64(0.4416918429003021), 'Recall': np.float64(0.797164667393675), 'F1 Score': np.float64(0.5684292379471229), 'Confusion': array([[1958,  924],\n",
      "       [ 186,  731]])}, 'LABEL_BaseExcess': {'Accuracy': 0.8373256120031587, 'Precision': np.float64(0.6487053883834849), 'Recall': np.float64(0.8887823585810163), 'F1 Score': np.float64(0.75), 'Confusion': array([[2254,  502],\n",
      "       [ 116,  927]])}, 'LABEL_Bilirubin_direct': {'Accuracy': 0.9010265859436694, 'Precision': np.float64(0.25101214574898784), 'Recall': np.float64(0.9538461538461539), 'F1 Score': np.float64(0.3974358974358974), 'Confusion': array([[3299,  370],\n",
      "       [   6,  124]])}, 'LABEL_Bilirubin_total': {'Accuracy': 0.7354566991313504, 'Precision': np.float64(0.4697272122421823), 'Recall': np.float64(0.7724288840262582), 'F1 Score': np.float64(0.5841952834091849), 'Confusion': array([[2088,  797],\n",
      "       [ 208,  706]])}, 'LABEL_EtCO2': {'Accuracy': 0.8989207686233219, 'Precision': np.float64(0.3874538745387454), 'Recall': np.float64(0.8015267175572519), 'F1 Score': np.float64(0.5223880597014925), 'Confusion': array([[3205,  332],\n",
      "       [  52,  210]])}, 'LABEL_Fibrinogen': {'Accuracy': 0.8410107923137667, 'Precision': np.float64(0.3084455324357405), 'Recall': np.float64(0.865979381443299), 'F1 Score': np.float64(0.4548736462093863), 'Confusion': array([[2943,  565],\n",
      "       [  39,  252]])}, 'LABEL_Lactate': {'Accuracy': 0.8239010265859437, 'Precision': np.float64(0.5548387096774193), 'Recall': np.float64(0.7639593908629442), 'F1 Score': np.float64(0.6428190069407368), 'Confusion': array([[2528,  483],\n",
      "       [ 186,  602]])}, 'LABEL_SaO2': {'Accuracy': 0.8073177151882074, 'Precision': np.float64(0.5700404858299595), 'Recall': np.float64(0.7779005524861878), 'F1 Score': np.float64(0.6579439252336449), 'Confusion': array([[2363,  531],\n",
      "       [ 201,  704]])}, 'LABEL_Sepsis': {'Accuracy': 0.8733877336141089, 'Precision': np.float64(0.29057187017001546), 'Recall': np.float64(0.8952380952380953), 'F1 Score': np.float64(0.4387397899649942), 'Confusion': array([[3130,  459],\n",
      "       [  22,  188]])}, 'LABEL_TroponinI': {'Accuracy': 0.8818110028954989, 'Precision': np.float64(0.4296875), 'Recall': np.float64(0.766016713091922), 'F1 Score': np.float64(0.5505505505505506), 'Confusion': array([[3075,  365],\n",
      "       [  84,  275]])}}\n",
      "RandomForest: {'LABEL_Alkalinephos': {'Accuracy': 0.8199526191102922, 'Precision': np.float64(0.5784023668639053), 'Recall': np.float64(0.8727678571428571), 'F1 Score': np.float64(0.6957295373665481), 'Confusion': array([[2333,  570],\n",
      "       [ 114,  782]])}, 'LABEL_AST': {'Accuracy': 0.7902079494603843, 'Precision': np.float64(0.5395256916996047), 'Recall': np.float64(0.8931297709923665), 'F1 Score': np.float64(0.6726899383983573), 'Confusion': array([[2183,  699],\n",
      "       [  98,  819]])}, 'LABEL_BaseExcess': {'Accuracy': 0.7491445117136089, 'Precision': np.float64(0.5253378378378378), 'Recall': np.float64(0.8945349952061361), 'F1 Score': np.float64(0.6619368570415041), 'Confusion': array([[1913,  843],\n",
      "       [ 110,  933]])}, 'LABEL_Bilirubin_direct': {'Accuracy': 0.9697288760200052, 'Precision': np.float64(0.5490196078431373), 'Recall': np.float64(0.6461538461538462), 'F1 Score': np.float64(0.5936395759717314), 'Confusion': array([[3600,   69],\n",
      "       [  46,   84]])}, 'LABEL_Bilirubin_total': {'Accuracy': 0.830218478546986, 'Precision': np.float64(0.6022813688212928), 'Recall': np.float64(0.8665207877461707), 'F1 Score': np.float64(0.7106325706594886), 'Confusion': array([[2362,  523],\n",
      "       [ 122,  792]])}, 'LABEL_EtCO2': {'Accuracy': 0.5967359831534614, 'Precision': np.float64(0.12778429073856976), 'Recall': np.float64(0.8320610687022901), 'F1 Score': np.float64(0.22154471544715448), 'Confusion': array([[2049, 1488],\n",
      "       [  44,  218]])}, 'LABEL_Fibrinogen': {'Accuracy': 0.7080810739668334, 'Precision': np.float64(0.18683001531393567), 'Recall': np.float64(0.8384879725085911), 'F1 Score': np.float64(0.3055729492798998), 'Confusion': array([[2446, 1062],\n",
      "       [  47,  244]])}, 'LABEL_Lactate': {'Accuracy': 0.6293761516188471, 'Precision': np.float64(0.34183673469387754), 'Recall': np.float64(0.850253807106599), 'F1 Score': np.float64(0.487627365356623), 'Confusion': array([[1721, 1290],\n",
      "       [ 118,  670]])}, 'LABEL_SaO2': {'Accuracy': 0.6649118188997104, 'Precision': np.float64(0.3994535519125683), 'Recall': np.float64(0.8077348066298342), 'F1 Score': np.float64(0.5345521023765997), 'Confusion': array([[1795, 1099],\n",
      "       [ 174,  731]])}, 'LABEL_Sepsis': {'Accuracy': 0.9705185575151356, 'Precision': np.float64(0.7722222222222223), 'Recall': np.float64(0.6619047619047619), 'F1 Score': np.float64(0.7128205128205128), 'Confusion': array([[3548,   41],\n",
      "       [  71,  139]])}, 'LABEL_TroponinI': {'Accuracy': 0.8470650171097658, 'Precision': np.float64(0.3483606557377049), 'Recall': np.float64(0.7103064066852368), 'F1 Score': np.float64(0.4674610449129239), 'Confusion': array([[2963,  477],\n",
      "       [ 104,  255]])}}\n"
     ]
    }
   ],
   "source": [
    "svc_metrics_file = paths[\"evaluation\"][\"svc\"]\n",
    "rf_metrics_file = paths[\"evaluation\"][\"random_forest\"]\n",
    "\n",
    "svc_metrics = compute_clf_metrics(X, X_val, y_val, clf_mask, svc_models, svc_metrics_file)\n",
    "rf_metrics = compute_clf_metrics(X, X_val, y_val, clf_mask, rf_models, rf_metrics_file)\n",
    "\n",
    "print(f\"SVC: {svc_metrics}\")\n",
    "print(f\"RandomForest: {rf_metrics}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d178d762ac8e4d3a8a2b871e92403541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Output(), Dropdown(description='Label:', options=('LABEL_Alkalinephos', 'LABEL_AST', 'LABEL_Bas…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interactive plot function\n",
    "def plot_metrics(label_name):\n",
    "\n",
    "    label_metrics = rf_metrics[label_name]\n",
    "    confusion_matrix = label_metrics[\"Confusion\"]\n",
    "\n",
    "    # Create a plot with two sections: heatmap and text\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(12, 6), gridspec_kw={\"width_ratios\": [1, 1]})\n",
    "\n",
    "    # Plot metrics for Random Forest\n",
    "    sns.heatmap(\n",
    "        confusion_matrix,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=[\"False\", \"True\"],\n",
    "        yticklabels=[\"False\", \"True\"],\n",
    "        ax=ax[0,0],\n",
    "    )\n",
    "    ax[0,0].set_title(f\"Confusion Matrix\")\n",
    "    ax[0,0].set_xlabel(\"Predicted\")\n",
    "    ax[0,0].set_ylabel(\"True\")\n",
    "\n",
    "    # Extract other metrics (accuracy, precision, recall, F1-score)\n",
    "    accuracy = label_metrics[\"Accuracy\"]\n",
    "    precision = label_metrics[\"Precision\"]\n",
    "    recall = label_metrics[\"Recall\"]\n",
    "    f1 = label_metrics[\"F1 Score\"]\n",
    "\n",
    "    # Prepare metrics text\n",
    "    metrics_text = (\n",
    "        f\"Accuracy:  {accuracy:.2f}\\n\"\n",
    "        f\"Precision: {precision:.2f}\\n\"\n",
    "        f\"Recall:    {recall:.2f}\\n\"\n",
    "        f\"F1 Score:  {f1:.2f}\"\n",
    "    )\n",
    "\n",
    "    # Add metrics text to the second subplot\n",
    "    ax[0,1].axis(\"off\")  # Turn off axis for text display\n",
    "    ax[0,1].text(\n",
    "        0.5,\n",
    "        0.5,\n",
    "        metrics_text,\n",
    "        fontsize=14,\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"white\", edgecolor=\"black\"),\n",
    "    )\n",
    "\n",
    "    # Plot metrics for SVC\n",
    "    label_metrics = svc_metrics[label_name]\n",
    "    confusion_matrix = label_metrics[\"Confusion\"]\n",
    "\n",
    "    sns.heatmap(\n",
    "        confusion_matrix,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=[\"False\", \"True\"],\n",
    "        yticklabels=[\"False\", \"True\"],\n",
    "        ax=ax[1,0],\n",
    "    )\n",
    "    ax[1,0].set_title(f\"Confusion Matrix\")\n",
    "    ax[1,0].set_xlabel(\"Predicted\")\n",
    "    ax[1,0].set_ylabel(\"True\")\n",
    "\n",
    "    # Extract other metrics (accuracy, precision, recall, F1-score)\n",
    "    accuracy = label_metrics[\"Accuracy\"]\n",
    "    precision = label_metrics[\"Precision\"]\n",
    "    recall = label_metrics[\"Recall\"]\n",
    "    f1 = label_metrics[\"F1 Score\"]\n",
    "\n",
    "    # Prepare metrics text\n",
    "    metrics_text = (\n",
    "        f\"Accuracy:  {accuracy:.2f}\\n\"\n",
    "        f\"Precision: {precision:.2f}\\n\"\n",
    "        f\"Recall:    {recall:.2f}\\n\"\n",
    "        f\"F1 Score:  {f1:.2f}\"\n",
    "    )\n",
    "\n",
    "    # Add metrics text to the second subplot\n",
    "    ax[1,1].axis(\"off\")  # Turn off axis for text display\n",
    "    ax[1,1].text(\n",
    "        0.5,\n",
    "        0.5,\n",
    "        metrics_text,\n",
    "        fontsize=14,\n",
    "        ha=\"center\",\n",
    "        va=\"center\",\n",
    "        bbox=dict(boxstyle=\"round\", facecolor=\"white\", edgecolor=\"black\"),\n",
    "    )\n",
    "\n",
    "    # Add row labels for \"Random Forest\" and \"SVC\"\n",
    "    ax[0, 0].text(\n",
    "        -0.5,\n",
    "        1.25,\n",
    "        \"Random Forest\",\n",
    "        transform=ax[0, 0].transAxes,\n",
    "        fontsize=14,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "    ax[1, 0].text(\n",
    "        -0.5, 1.25, \"SVC\", transform=ax[1, 0].transAxes, fontsize=14, fontweight=\"bold\"\n",
    "    )\n",
    "\n",
    "    fig.suptitle(f\"Classification metrics comparison of {label_name}\")\n",
    "\n",
    "    # Display the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Create interactive selector at the bottom\n",
    "label_selector = widgets.Dropdown(\n",
    "    options=list(svc_metrics.keys()),\n",
    "    description=\"Label:\",\n",
    "    style={\"description_width\": \"initial\"},\n",
    ")\n",
    "out = widgets.interactive_output(plot_metrics, {\"label_name\": label_selector})\n",
    "ui = VBox([out, label_selector])\n",
    "display(ui)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vitals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
